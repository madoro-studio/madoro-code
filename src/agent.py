"""
MADORO CODE - ì—ì´ì „íŠ¸ ë£¨í”„

ì‘ë™ í”Œë¡œìš°:
1. ì‚¬ìš©ì ìš”ì²­
2. ì»¨í…ìŠ¤íŠ¸ íŒ© ìƒì„± (SSOT + ê´€ë ¨ íŒŒì¼ + ìµœê·¼ ëŒ€í™”)
3. LLMì´ íˆ´ì½œ(JSON)ë¡œ íŒ¨ì¹˜ ìƒì„±
4. Executorê°€ íŒ¨ì¹˜ ì ìš©
5. í…ŒìŠ¤íŠ¸ ì‹¤í–‰
6. ë¡œê·¸ ê¸°ë¡ â†’ ë‹¤ì‹œ ëª¨ë¸ì— í”¼ë“œë°±
"""

import json
from typing import Optional, List, Dict, Any
from dataclasses import dataclass

from memory import get_memory_store
from llm import get_llm_client, LLMResponse
from tools import ToolExecutor, TOOL_DEFINITIONS, ToolResult
from context import get_context_builder, ContextPack


@dataclass
class AgentResponse:
    """ì—ì´ì „íŠ¸ ì‘ë‹µ"""
    message: str
    tool_results: List[Dict] = None
    error: Optional[str] = None


class Agent:
    """MADORO CODE ì—ì´ì „íŠ¸"""

    SYSTEM_PROMPT = """You are MADORO CODE, a coding assistant.

Core Principles:
1. Memory is managed by the system. Don't try to remember entire conversation history.
2. Only reference the provided context (SSOT docs, related files, recent conversation).
3. Use tools when file modifications are needed.
4. Don't guess - use the search tool when you need to find something.

Response Rules:
- Respond in the same language the user uses
- Use apply_patch tool for code modifications
- Use run_tests tool when testing is needed
- If the user pastes content directly, analyze it immediately without using file read tools
- Only use read_file tool when the user mentions a file path without providing content
- Avoid unnecessary tool calls: respond directly if the user already provided the information
"""

    MAX_ITERATIONS = 5  # ìµœëŒ€ íˆ´ì½œ ë°˜ë³µ íšŸìˆ˜

    def __init__(self, project_root: str = ".", progress_callback=None,
                 ssot_approval_callback=None):
        self.project_root = project_root
        self.memory = get_memory_store()
        self.llm = get_llm_client()
        self.tools = ToolExecutor(project_root, ssot_approval_callback=ssot_approval_callback)
        self.context_builder = get_context_builder(project_root)
        self.progress_callback = progress_callback  # Progress callback
        self.ssot_approval_callback = ssot_approval_callback  # SSOT file approval callback

    def _report_progress(self, status: str, detail: str = ""):
        """Report progress"""
        print(f"[Agent] {status}: {detail}")
        if self.progress_callback:
            self.progress_callback(status, detail)

    def process(self, user_input: str) -> AgentResponse:
        """Process user input"""
        self._report_progress("Starting", user_input[:50])

        # ëŒ€í™” í„´ ê¸°ë¡
        self.memory.add_turn("user", user_input)

        # Build context pack
        self._report_progress("Building context", "Loading project state...")
        context_pack = self.context_builder.build(
            task=user_input,
            query=self._extract_search_query(user_input)
        )
        self._report_progress("Context ready", f"{len(context_pack.project_state)} chars")

        # LLM í˜¸ì¶œ (íˆ´ì½œ í¬í•¨)
        all_tool_results = []
        final_response = None

        for iteration in range(self.MAX_ITERATIONS):
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = self._build_prompt(user_input, context_pack, all_tool_results)

            # í˜„ì¬ ëª¨ë¸ëª… ê°€ì ¸ì˜¤ê¸°
            model_cfg = self.llm.get_model_config()
            model_name = model_cfg.display_name if model_cfg else "LLM"
            self._report_progress("LLM call", f"Waiting for {model_name}...")

            try:
                response = self.llm.generate_with_tools(
                    prompt=prompt,
                    tools=TOOL_DEFINITIONS,
                    system=self.SYSTEM_PROMPT
                )
                self._report_progress("LLM response", f"Received {len(response.content)} chars")
            except Exception as e:
                self._report_progress("LLM error", str(e))
                return AgentResponse(
                    message="",
                    error=f"LLM call failed: {e}"
                )

            # íˆ´ì½œ ì²˜ë¦¬
            if response.tool_calls:
                for tool_call in response.tool_calls:
                    tool_name = tool_call.get("tool", "")
                    tool_args = tool_call.get("args", {})

                    # Show tool execution status
                    tool_detail = self._get_tool_detail(tool_name, tool_args)
                    self._report_progress("Running tool", f"{tool_name}: {tool_detail}")

                    result = self.tools.execute(tool_name, tool_args)
                    status = "âœ“" if result.success else "âœ—"
                    self._report_progress("Tool done", f"{status} {tool_name}")

                    all_tool_results.append({
                        "tool": tool_name,
                        "args": tool_args,
                        "success": result.success,
                        "output": result.output[:500],
                        "error": result.error
                    })

                # Auto-run tests after patch applied
                if any(tc.get("tool") == "apply_patch" and tc.get("success")
                       for tc in all_tool_results):
                    self._report_progress("Running tests", "Executing pytest...")
                    test_result = self.tools.execute("run_tests", {"cmd": "pytest -q"})
                    status = "âœ“ Passed" if test_result.success else "âœ— Failed"
                    self._report_progress("Tests done", status)
                    all_tool_results.append({
                        "tool": "run_tests (auto)",
                        "success": test_result.success,
                        "output": test_result.output[:300]
                    })
            else:
                # íˆ´ì½œ ì—†ìœ¼ë©´ ìµœì¢… ì‘ë‹µ
                final_response = response.content
                break

        if not final_response:
            # Generate final response after tool iterations
            self._report_progress("Generating", "Summarizing results...")
            summary_prompt = self._build_summary_prompt(
                user_input, context_pack, all_tool_results
            )
            try:
                response = self.llm.generate(summary_prompt, system=self.SYSTEM_PROMPT)
                final_response = response.content
            except Exception as e:
                final_response = f"Task complete. (Summary generation failed: {e})"

        self._report_progress("Complete", "")

        # ì‘ë‹µ ê¸°ë¡
        self.memory.add_turn("assistant", final_response[:500])

        # ì‘ì—… ë¡œê·¸
        self.memory.log_work(
            action="CHAT",
            target="agent",
            description=user_input[:100],
            result="SUCCESS",
            details={
                "tool_calls": len(all_tool_results),
                "response_length": len(final_response)
            }
        )

        return AgentResponse(
            message=final_response,
            tool_results=all_tool_results
        )

    def _get_tool_detail(self, tool_name: str, args: Dict) -> str:
        """ë„êµ¬ ì‹¤í–‰ ìƒì„¸ ì •ë³´ ìƒì„±"""
        if tool_name == "read_file":
            return args.get("path", "")[:50]
        elif tool_name == "search":
            return f'"{args.get("query", "")}"'
        elif tool_name == "apply_patch":
            files = args.get("files", [])
            if files:
                return f"{len(files)}ê°œ íŒŒì¼"
            return ""
        elif tool_name == "run_tests":
            return args.get("cmd", "pytest")[:30]
        elif tool_name == "list_files":
            return args.get("path", ".")[:30]
        elif tool_name == "get_diff":
            return "git ë³€ê²½ì‚¬í•­"
        return ""

    def _extract_search_query(self, user_input: str) -> str:
        """ì‚¬ìš©ì ì…ë ¥ì—ì„œ ê²€ìƒ‰ ì¿¼ë¦¬ ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = []
        for word in user_input.split():
            if len(word) > 2 and not word.startswith(('ì´', 'ê·¸', 'ì €', 'ë­', 'ì–´ë–»')):
                keywords.append(word)
        return ' '.join(keywords[:3])

    def _build_prompt(self, user_input: str, context: ContextPack,
                      tool_results: List[Dict]) -> str:
        """LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        parts = []

        # ì»¨í…ìŠ¤íŠ¸
        parts.append(context.to_prompt())

        # ì´ì „ íˆ´ ê²°ê³¼
        if tool_results:
            parts.append("[TOOL RESULTS]")
            for tr in tool_results[-3:]:  # ìµœê·¼ 3ê°œë§Œ
                status = "âœ…" if tr.get("success") else "âŒ"
                parts.append(f"{status} {tr.get('tool')}: {tr.get('output', '')[:200]}")
            parts.append("")

        # ì‚¬ìš©ì ìš”ì²­
        parts.append("[USER REQUEST]")
        parts.append(user_input)

        return "\n".join(parts)

    def _build_summary_prompt(self, user_input: str, context: ContextPack,
                              tool_results: List[Dict]) -> str:
        """ìµœì¢… ìš”ì•½ í”„ë¡¬í”„íŠ¸"""
        parts = []

        parts.append("ë‹¤ìŒ ì‘ì—…ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.")
        parts.append("")
        parts.append(f"[ìš”ì²­] {user_input}")
        parts.append("")
        parts.append("[ìˆ˜í–‰ëœ ì‘ì—…]")
        for tr in tool_results:
            status = "ì„±ê³µ" if tr.get("success") else "ì‹¤íŒ¨"
            parts.append(f"- {tr.get('tool')}: {status}")
            if tr.get("error"):
                parts.append(f"  ì˜¤ë¥˜: {tr.get('error')}")

        return "\n".join(parts)

    def doctor(self) -> str:
        """í”„ë¡œì íŠ¸ ìƒíƒœ ì§„ë‹¨ (vibe doctor)"""
        context = self.context_builder.build(task="í”„ë¡œì íŠ¸ ìƒíƒœ ì ê²€")

        report = []
        report.append("=" * 60)
        report.append("  MADORO CODE Doctor - í”„ë¡œì íŠ¸ ìƒíƒœ ë¦¬í¬íŠ¸")
        report.append("=" * 60)
        report.append("")

        # í”„ë¡œì íŠ¸ ìƒíƒœ
        report.append("[ğŸ“‹ í”„ë¡œì íŠ¸ ìƒíƒœ]")
        # HANDOVER.mdì—ì„œ í˜„ì¬ ìƒíƒœ ì¶”ì¶œ
        if "í˜„ì¬ ìƒíƒœ" in context.project_state:
            for line in context.project_state.split('\n'):
                if '|' in line and ('ë²„ì „' in line or 'ë‹¨ê³„' in line or 'ì‘ì—…' in line):
                    report.append(f"  {line.strip()}")
        report.append("")

        # ì—´ë¦° ì´ìŠˆ
        report.append("[ğŸ› ì—´ë¦° ì´ìŠˆ]")
        if context.open_issues:
            for issue in context.open_issues:
                report.append(f"  [{issue['severity']}] {issue['title']}")
        else:
            report.append("  ì—†ìŒ")
        report.append("")

        # ìµœê·¼ ë³€ê²½
        report.append("[ğŸ“ ìµœê·¼ ë³€ê²½]")
        if context.recent_changes and context.recent_changes != "(No git history)":
            for line in context.recent_changes.split('\n')[:5]:
                report.append(f"  {line}")
        else:
            report.append("  ë³€ê²½ ì—†ìŒ")
        report.append("")

        # ìµœê·¼ ëŒ€í™”
        report.append("[ğŸ’¬ ìµœê·¼ ëŒ€í™”]")
        if context.recent_turns:
            for turn in context.recent_turns[-3:]:
                content = turn['content'][:50] + "..." if len(turn['content']) > 50 else turn['content']
                report.append(f"  [{turn['role']}] {content}")
        else:
            report.append("  ëŒ€í™” ì—†ìŒ")
        report.append("")

        # ëª¨ë¸ ìƒíƒœ
        report.append("[ğŸ¤– ëª¨ë¸ ìƒíƒœ]")
        report.append(f"  í˜„ì¬ ëª¨ë¸: {self.llm.current_model}")
        connected = self.llm.check_connection()
        report.append(f"  Ollama ì—°ê²°: {'âœ… ì •ìƒ' if connected else 'âŒ ì—°ê²° ì•ˆë¨'}")
        if connected:
            for model_key in self.llm.list_models():
                available = self.llm.check_model_available(model_key)
                cfg = self.llm.models[model_key]
                status = "âœ…" if available else "âŒ"
                report.append(f"  {status} {cfg.display_name}")
        report.append("")

        report.append("=" * 60)

        return "\n".join(report)


# ============================================
# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
# ============================================

_agent: Optional[Agent] = None


def get_agent(project_root: str = ".") -> Agent:
    """ì—ì´ì „íŠ¸ ì‹±ê¸€í†¤"""
    global _agent
    if _agent is None:
        _agent = Agent(project_root)
    return _agent


# ============================================
# í…ŒìŠ¤íŠ¸
# ============================================

if __name__ == "__main__":
    print("=" * 60)
    print("  MADORO CODE Agent Test")
    print("=" * 60)

    agent = Agent(".")

    print("\n[1] Doctor")
    print(agent.doctor())

    print("\n[2] Connection Check")
    if agent.llm.check_connection():
        print("  Ollama connected!")

        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
        print("\n[3] Simple Request")
        response = agent.process("í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ ëª©ë¡ì„ ë³´ì—¬ì¤˜")
        print(f"  Response: {response.message[:200]}...")
        if response.tool_results:
            print(f"  Tool calls: {len(response.tool_results)}")
    else:
        print("  âŒ Ollama not connected. Start Ollama first.")

    print("\n" + "=" * 60)
